#!/usr/bin/python

import sys, os, argparse, json
from multiprocessing import Process, JoinableQueue, cpu_count
from subprocess import Popen, PIPE
from signal import signal, SIGINT, SIG_IGN

DESCRIPTION="""
A utility to help parse results from the Shadow simulator.

This script enables processing of shadow log files and storing processed
data in json format for plotting. It was written so that the log file
need never be stored on disk decompressed, which is useful when log file
sizes reach tens of gigabytes.

Use the help menu to understand usage:
$ python parse-shadow.py -h

The standard way to run the script is to give the shadow.log file as
a positional argument:
$ python parse-shadow.py shadow.log
$ python parse-shadow.py shadow.log.xz

The log data can also be passed on STDIN with the special '-' filename:
$ cat shadow.log | python parse-shadow.py -
$ xzcat shadow.log.xz | python parse-shadow.py -

The default mode is to filter and parse the log file using a single
process; this will be done with multiple worker processes when passing
the '-m' option.

Another good way to run this script is to use grep to do the filtering
and then just run in single process mode consuming the log from STDIN:
$ xzcat shadow.log.xz | grep -e 'shadow-heartbeat' -e 'fg-download-complete' 
-e 'torctl-log' -e 'transfer-complete' | python parse-shadow.py -\n
"""

NUMFILTERWORKERS=4
FILTERBATCHLEN=10000 # number of lines per put() from master
MAXFILTERQLEN=100 # number of line batch items
PROCBATCHLEN=1000 # number of split-lines per put() from filter worker
MAXPROCQLEN=10 # number of split-line batch items per processor worker

SHADOWJSON="stats.shadow.json"
TORJSON="stats.tor.json"
FILETRANSFERJSON="stats.filetransfer.json"
TGENJSON="stats.tgen.json"

def main():
    parser = argparse.ArgumentParser(
        description=DESCRIPTION, 
        formatter_class=argparse.RawTextHelpFormatter)#ArgumentDefaultsHelpFormatter)

    parser.add_argument(
        help="""The PATH to the shadow.log file, which may be '-'
for STDIN, or may end in '.xz' to enable inline
xz decompression""", 
        metavar="PATH",
        action="store", dest="logpath")

    parser.add_argument('-m', '--multiproc',
        help="""Enable multiprocessing""", 
        action="store_true", dest="multiproc",
        default=False)

    parser.add_argument('-p', '--prefix', 
        help="""a STRING directory path prefix where the processed
data files generated by this script will be written""", 
        metavar="STRING",
        action="store", dest="prefix",
        default=os.getcwd())

    parser.add_argument('-t', '--tee',
        help="""Echo decompressed log input back to stdout""", 
        action="store_true", dest="tee",
        default=False)

    parser.add_argument('-s', '--skip',
        help="""Ignore the first N seconds of the simulation while parsing""", 
        metavar="N",
        action="store", dest="skiptime", type=int,
        default=0)

    parser.add_argument('--enable-shadow',
        help="""Enable parsing of Shadow packet stats (high RAM consumption)""", 
        action="store_true", dest="doshadow",
        default=False)

    parser.add_argument('--disable-filetransfer',
        help="""Disable parsing of filetransfer downloads""", 
        action="store_false", dest="dofiletransfer",
        default=True)

    parser.add_argument('--disable-tgen',
        help="""Disable parsing of tgen transfers""", 
        action="store_false", dest="dotgen",
        default=True)

    parser.add_argument('--disable-tor',
        help="""Disable parsing of Tor throughput""", 
        action="store_false", dest="dotor",
        default=True)

    args = parser.parse_args()
    args.logpath = os.path.abspath(os.path.expanduser(args.logpath))
    args.prefix = os.path.abspath(os.path.expanduser(args.prefix))

    if args.logpath.endswith("-"):
        args.datasource = sys.stdin
    elif args.logpath.endswith(".xz"):
        args.xzproc = Popen(["xz", "--decompress", "--stdout", args.logpath], stdout=PIPE)
        args.datasource = args.xzproc.stdout
    else: args.datasource = open(args.logpath, 'r')

    if args.multiproc: mutli_parse(args)
    else: single_parse(args)

def single_parse(args):
    sdata, tdata, fdata, tgendata = {}, {}, {}, {}
    for line in args.datasource:
        try:
            if args.tee: sys.stdout.write(line)
            parts = line.strip('\n').split(' ')
            # do not process if timestamp is not intact
            if len(parts) < 3 or 'n/a' in parts[2] or parts[2].find(':') < 0: continue 
            # dont include bootstrapping period
            if parsetimestamp(parts[2]) < args.skiptime: continue
            # check for our log line types
            if 'slave_heartbeat' in parts[5] or (args.doshadow and 'shadow-heartbeat' in parts[6]): shadow_helper(sdata, parts)
            elif args.dotor and 'torctl-log' in parts[6]: tor_helper(tdata, parts)
            elif args.dofiletransfer and 'fg-download-complete' in parts[6]: filetransfer_helper(fdata, parts)
            elif args.dotgen and ('transfer-complete' in parts[6] or 'transfer-error' in parts[6]): tgen_helper(tgendata, parts)
        except: continue
    if 'xzproc' in args: args.xzproc.wait()
    if len(sdata) > 0: dump(args, sdata, SHADOWJSON)
    if len(tdata) > 0: dump(args, tdata, TORJSON)
    if len(fdata) > 0: dump(args, fdata, FILETRANSFERJSON)
    if len(tgendata) > 0: dump(args, tgendata, TGENJSON)

def mutli_parse(args):
     # create queues for inter-process communication
    shadowQ = JoinableQueue(MAXPROCQLEN)
    torQ = JoinableQueue(MAXPROCQLEN)
    filetransferQ = JoinableQueue(MAXPROCQLEN)
    tgenQ = JoinableQueue(MAXPROCQLEN)
    filterQ = JoinableQueue(MAXFILTERQLEN)

    # create the worker processes
    pool = []
    pool.append(Process(target=shadowworker_main, args=(args, shadowQ)))
    pool.append(Process(target=torworker_main, args=(args, torQ)))
    pool.append(Process(target=filetransferworker_main, args=(args, filetransferQ)))
    pool.append(Process(target=tgenworker_main, args=(args, tgenQ)))
    for i in xrange(NUMFILTERWORKERS): pool.append(Process(target=filterworker_main, args=(args, filterQ, shadowQ, torQ, filetransferQ, tgenQ)))

    # start the workers
    for p in pool: p.start()
    print >> sys.stderr, "workers started, processing input..."

    # if ctrl-c is pressed, shutdown child processes properly
    try:
        # read lines in and pass them to the filter workers
        linebatch = []
        for line in args.datasource:
            if args.tee: sys.stdout.write(line)
            linebatch.append(line)
            if len(linebatch) >= FILTERBATCHLEN:
                filterQ.put(linebatch)
                linebatch = []
    except KeyboardInterrupt:
        print >> sys.stderr, "\ninterrupt received\nplease wait while we cleanup child processes..."        
        if 'xzproc' in args:
            args.xzproc.terminate()
            args.xzproc.wait()
        for p in pool: p.terminate()
        for p in pool: p.join()
        print >> sys.stderr, "cleanup complete!\nexiting..."
        sys.exit()

    print >> sys.stderr, "dumping processed data"
    # xz is done sending us file contents
    if 'xzproc' in args: args.xzproc.wait()
    # tell the filter processes to quit when all lines were filtered
    for i in xrange(NUMFILTERWORKERS): filterQ.put('STOP')
    filterQ.join()
    # now that all lines have been filtered, stop the processing helpers
    for Q in [filetransferQ, torQ, shadowQ, tgenQ]: Q.put('STOP')
    for Q in [filetransferQ, torQ, shadowQ, tgenQ]: Q.join()

def filterworker_main(args, filterQ, shadowQ, torQ, filetransferQ, tgenQ):
    signal(SIGINT, SIG_IGN) # ignore interrupts
    shadowbatch, torbatch, filetransferbatch, tgenbatch = [], [], [], []
    for linebatch in iter(filterQ.get, 'STOP'):
        for line in linebatch:
            try: # skip malformed lines
                parts = line.strip('\n').split(' ')
                # do not process if timestamp is not intact
                if len(parts) < 3 or 'n/a' in parts[2] or parts[2].find(':') < 0: raise 
                # dont include bootstrapping period
                if parsetimestamp(parts[2]) < args.skiptime: raise
                # check for our log line types
                if 'slave_heartbeat' in parts[5] or (args.doshadow and 'shadow-heartbeat' in parts[6]): shadowbatch.append(parts)
                elif args.dotor and 'torctl-log' in parts[6]: torbatch.append(parts)
                elif args.dofiletransfer and 'fg-download-complete' in parts[6]: filetransferbatch.append(parts)
                elif args.dotgen and ('transfer-complete' in parts[6] or 'transfer-error' in parts[6]): tgenbatch.append(parts)
            except: pass
            # send out full batches
            if len(shadowbatch) >= PROCBATCHLEN:
                shadowQ.put(shadowbatch)
                shadowbatch = []
            if len(torbatch) >= PROCBATCHLEN:
                torQ.put(torbatch)
                torbatch = []
            if len(filetransferbatch) >= PROCBATCHLEN:
                filetransferQ.put(filetransferbatch)
                filetransferbatch = []
            if len(tgenbatch) >= PROCBATCHLEN:
                tgenQ.put(tgenbatch)
                tgenbatch = []
        filterQ.task_done() # finished a line batch
    # send out partial batches
    if len(shadowbatch) > 0: shadowQ.put(shadowbatch)
    if len(torbatch) > 0: torQ.put(torbatch)
    if len(filetransferbatch) > 0: filetransferQ.put(filetransferbatch)
    if len(tgenbatch) > 0: tgenQ.put(tgenbatch)
    # tell them we are done feeding
    shadowQ.put('STOP')
    torQ.put('STOP')
    filetransferQ.put('STOP')
    tgenQ.put('STOP')
    filterQ.task_done() # got the stop command, we are done

def shadowworker_main(args, partsQ): # process target function
    worker_main_helper(args, partsQ, shadow_helper, SHADOWJSON)

def torworker_main(args, partsQ): # process target function
    worker_main_helper(args, partsQ, tor_helper, TORJSON)

def filetransferworker_main(args, partsQ): # process target function
    worker_main_helper(args, partsQ, filetransfer_helper, FILETRANSFERJSON)

def tgenworker_main(args, partsQ): # process target function
    worker_main_helper(args, partsQ, tgen_helper, TGENJSON)

def worker_main_helper(args, partsQ, helperfunc, dumpfilename):
    signal(SIGINT, SIG_IGN) # ignore interrupts
    data, stopcount = {}, 0
    while stopcount < NUMFILTERWORKERS+1:
        for partbatch in iter(partsQ.get, 'STOP'):
            for parts in partbatch: helperfunc(data, parts)
            partsQ.task_done()
        stopcount += 1
    if len(data) > 0: dump(args, data, dumpfilename)
    for i in xrange(NUMFILTERWORKERS+1): partsQ.task_done()

'''
[node-header] interval-seconds,recv-bytes,send-bytes,cpu-percent,delayed-count,avgdelay-milliseconds;inbound-localhost-counters;outbound-localhost-counters;inbound-remote-counters;outbound-remote-counters
where counters are:
total-packets,total-bytes,payload-bytes,header-bytes,payload-packets,payload-header-bytes,control-packets,control-header-bytes,retrans-packets,retrans-header-bytes,retrans-payload-bytes
'''
def shadow_helper(d, parts):
    if len(parts) < 9: return

    virtualts = parsetimestamp(parts[2])
    second = int(virtualts)

    if 'slave_heartbeat' in parts[5]:
        realts = parsetimestamp(parts[0])
        maxrss = float(parts[12].split('=')[1]) if 'maxrss' in parts[12] else -1.0

        if 'ticks' not in d: d['ticks'] = {}
        if second not in d['ticks']: d['ticks'][second] = {'time_seconds':realts, 'maxrss_gib':maxrss}
        return

    # now this must be a shadow-heartbeat message
    if '[node]' != parts[7]: return
    name = parts[4].lstrip('[').rstrip(']') # eg: [webclient2-11.0.5.99]

    mods = parts[8].split(';')
    #nodestats = mods[0].split(',')
    #localin = mods[1].split(',')
    #localout = mods[2].split(',')
    remotein = mods[3].split(',')
    remoteout = mods[4].split(',')

    #labels = ['count_total', 'count_data', 'count_control', 'count_retrans', 'bytes_total', 'bytes_data', 'bytes_control', 'bytes_retrans']
    labels = ['bytes_total', 'bytes_control_header', 'bytes_control_header_retrans', 'bytes_data_header', 'bytes_data_payload', 'bytes_data_header_retrans', 'bytes_data_payload_retrans']

    if 'nodes' not in d: d['nodes'] = {}
    if name not in d['nodes']:
        d['nodes'][name] = {'recv':{}, 'send':{}}
        for label in labels:
            d['nodes'][name]['recv'][label] = {}
            d['nodes'][name]['send'][label] = {}
    for label in labels:
        if second not in d['nodes'][name]['recv'][label]: d['nodes'][name]['recv'][label][second] = 0
        if second not in d['nodes'][name]['send'][label]: d['nodes'][name]['send'][label][second] = 0

    '''
    a packet is a data packet if it contains a payload, and a control packet otherwise.
    each packet potentially has a header and a payload, and each packet is either
    a first transmission or a re-transmission.

    shadow prints the following in its heartbeat messages for the bytes counters:
    packets-total,bytes-total,
    packets-control,bytes-control-header,
    packets-control-retrans,bytes-control-header-retrans,
    packets-data,bytes-data-header,bytes-data-payload,
    packets-data-retrans,bytes-data-header-retrans,bytes-data-payload-retrans
    '''
    # packet counts are also available, but we are ignoring them
    d['nodes'][name]['recv']['bytes_total'][second] += int(remotein[1])
    d['nodes'][name]['recv']['bytes_control_header'][second] += int(remotein[3])
    d['nodes'][name]['recv']['bytes_control_header_retrans'][second] += int(remotein[5])
    d['nodes'][name]['recv']['bytes_data_header'][second] += int(remotein[7])
    d['nodes'][name]['recv']['bytes_data_payload'][second] += int(remotein[8])
    d['nodes'][name]['recv']['bytes_data_header_retrans'][second] += int(remotein[10])
    d['nodes'][name]['recv']['bytes_data_payload_retrans'][second] += int(remotein[11])

    d['nodes'][name]['send']['bytes_total'][second] += int(remoteout[1])
    d['nodes'][name]['send']['bytes_control_header'][second] += int(remoteout[3])
    d['nodes'][name]['send']['bytes_control_header_retrans'][second] += int(remoteout[5])
    d['nodes'][name]['send']['bytes_data_header'][second] += int(remoteout[7])
    d['nodes'][name]['send']['bytes_data_payload'][second] += int(remoteout[8])
    d['nodes'][name]['send']['bytes_data_header_retrans'][second] += int(remoteout[10])
    d['nodes'][name]['send']['bytes_data_payload_retrans'][second] += int(remoteout[11])

def tor_helper(d, parts):
    if 'BW' != parts[9]: return

    virtualts = parsetimestamp(parts[2])
    name = parts[4].lstrip('[').rstrip(']') # eg: [4uthority1-82.94.251.203]

    bwr = int(parts[10])
    bww = int(parts[11])
    second = int(virtualts)

    if 'nodes' not in d: d['nodes'] = {}
    if name not in d['nodes']: d['nodes'][name] = {'bytes_read':{}, 'bytes_written':{}}
    if second not in d['nodes'][name]['bytes_read']: d['nodes'][name]['bytes_read'][second] = 0
    if second not in d['nodes'][name]['bytes_written']: d['nodes'][name]['bytes_written'][second] = 0

    d['nodes'][name]['bytes_read'][second] += bwr
    d['nodes'][name]['bytes_written'][second] += bww

def filetransfer_helper(d, parts):
    name = parts[4].lstrip('[').rstrip(']') # eg: [webclient2-11.0.5.99]

    fbtime = float(parts[11])
    bytes = int(parts[16])
    lbtime = float(parts[19])

    if 'nodes' not in d: d['nodes'] = {}
    if name not in d['nodes']: d['nodes'][name] = {}
    if bytes not in d['nodes'][name]: d['nodes'][name][bytes] = {'firstbyte':[], 'lastbyte':[]}

    d['nodes'][name][bytes]['firstbyte'].append(fbtime)
    d['nodes'][name][bytes]['lastbyte'].append(lbtime)

def tgen_helper(d, parts):
    name = parts[4].lstrip('[').rstrip(']')

    ioparts = parts[13].split('=')
    iodirection = ioparts[0]
    if 'read' not in iodirection: return

    bytes = int(ioparts[1].split('/')[0])

    if 'nodes' not in d: d['nodes'] = {}
    if name not in d['nodes']: d['nodes'][name] = {'firstbyte':{}, 'lastbyte':{}, 'errors':{}}

    if 'transfer-complete' in parts[6]:
        cmdtime = int(parts[15].split('=')[1])/1000.0
        rsptime = int(parts[16].split('=')[1])/1000.0
        fbtime = int(parts[17].split('=')[1])/1000.0
        lbtime = int(parts[18].split('=')[1])/1000.0
        chktime = int(parts[19].split('=')[1])/1000.0

        if bytes not in d['nodes'][name]['firstbyte']: d['nodes'][name]['firstbyte'][bytes] = []
        d['nodes'][name]['firstbyte'][bytes].append(fbtime-cmdtime)

        if bytes not in d['nodes'][name]['lastbyte']: d['nodes'][name]['lastbyte'][bytes] = []
        d['nodes'][name]['lastbyte'][bytes].append(lbtime-cmdtime)

    elif 'transfer-error' in parts[6]:
        code = parts[10].strip('()').split('-')[7].split('=')[1]

        if code not in d['nodes'][name]['errors']: d['nodes'][name]['errors'][code] = []
        d['nodes'][name]['errors'][code].append(bytes)

def dump(args, data, filename, compress=True):
    if not os.path.exists(args.prefix): os.makedirs(args.prefix)
    if compress: # inline compression
        path = "{0}/{1}.xz".format(args.prefix, filename)
        xzp = Popen(["xz", "--threads=3", "-"], stdin=PIPE, stdout=PIPE)
        ddp = Popen(["dd", "status=none", "of={0}".format(path)], stdin=xzp.stdout)
        json.dump(data, xzp.stdin, sort_keys=True, separators=(',', ': '), indent=2)
        xzp.stdin.close()
        xzp.wait()
        ddp.wait()
    else: # no compression
        path = "{0}/{1}".format(args.prefix, filename)
        with open(path, 'w') as outf: json.dump(data, outf, sort_keys=True, separators=(',', ': '), indent=2)

# helper - parse shadow timestamps
def parsetimestamp(stamp):
    parts = stamp.split(":")
    h, m, s = int(parts[0]), int(parts[1]), float(parts[2])
    seconds = h*3600.0 + m*60.0 + s
    return seconds

if __name__ == '__main__': sys.exit(main())

